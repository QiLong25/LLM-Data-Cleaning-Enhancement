# LLM-Data-Cleaning-Enhancement

### Organization: [Center For Data Science, Zhejiang University](http://cds.zju.edu.cn/)

![model](https://github.com/QiLong25/LLM-Data-Cleaning-Enhancement/assets/143149589/08da5491-e331-464f-bbdc-4537066d4897)

## Team Member
**(Team Leader) Qi Long**, Bingjun Guo, Yuxuan Li

## Abstract
To enhance the data cleaning process on huge data nowadays, our team proposed an innovative **representation learning** method to improve data utility efficiency and the performance on the wide-range down stream tasks. Our proposal aimed at dealing with missing data, dirty data and normal data representation at the same time, and taking missing data and dirty data into consideration along with normal data to even enhance the model performance on down stream tasks. Specifically, our model is designed based on **RAHA error detection model** and **SAINT tabular data deep learning model**. Besides, applying **Large Language Model (LLM)** into this whole industrial pipeline is another outstanding attempt in tabular data area. The experiment results showed that our model can fully use missing data, dirty data and normal data in a whole as it reaches a state-of-art performance on tested downstream tasks, and thus retains the potential to outperform the original method in all means. Meanwhile, due to the limit time and computing resources, some judgments and deeper designs still requires substantiated with further experiments.

## Core Work
 *  **Paper Research**: RAHA, BARAN, Transformer, BERT, TabNet, SAINT

 *  **Baseline Evaluation**: TabNet, SAINT

 *  **Model Proposal & Implementation**: RAHA+SAINT, RAHA+New-SAINT Pretrain

## Start Here
See **Poster.pdf** for an overview of our research.

See **Report.pdf** for detailed information of our research.

See **Source Code** for detailed model implementation.
